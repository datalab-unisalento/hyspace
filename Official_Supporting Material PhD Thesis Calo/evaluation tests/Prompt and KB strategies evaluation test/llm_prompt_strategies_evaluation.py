# -*- coding: utf-8 -*-
"""LLM_Prompt_Strategies_Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NJqYOSqtS_bv0LDcdBrAaW3X1yMvRZRQ

# LMM behavior analysis through the comparison of results across prompt implemtations and taxonomies simulations

The present notebook provides the codes to calculate measures with comments before to explain what was measured, and after, to discuss results.

These measures were selected not to assess the quality of the obtained outcome, so if the classified labels are trustworthy or not, but to evaluate, without a golden standard, lmm terminological and logical structural coherence objectively.

To assess if any of the obtained classifications are true, a golden standard would be needed.

The present evaluation focuses only on threats classification. Linguistic cues were tested preliminarily and are assessed in a separate notebook

# Terminological Fragmentation
After having extracted gdelt texts, we used llama 3 3.70B from JRC to extract the attack type with a simple prompt.

After having extracted GDELT texts, we used Llama 3 70B to extract the attack type with a simple prompt. This step evaluates terminological fragmentation in the model outputs when no controlled vocabulary is enforced. The analysis is performed on the raw attack_type labels exactly as produced by the model, in order to measure the intrinsic instability of free-form prompting.Scientific Metrics Employed:Number of unique labels and Unique/N ratio: Measure how many distinct labels are generated relative to the number of classified events. High values indicate label proliferation.Shannon Entropy ($H$): Derived from Information Theory, it quantifies the uncertainty or "surprise" associated with the label distribution ($H(X) = -\sum P(x_i) \log_2 P(x_i)$). In this context, it measures the complexity of the vocabulary: a higher entropy indicates that the LLM is constantly introducing new terms rather than converging.Normalized Entropy (0–1): Also known as Efficiency, it is the ratio of actual entropy to the maximum possible entropy ($\frac{H}{H_{max}}$). This allows us to compare the "disorder" of datasets with different sizes (e.g., $N=622$ vs $N=81$) by scaling the result to a universal 0-1 range.Top-10 Coverage: Measures how much of the dataset is concentrated in the top ten frequent labels.Note on Early Testing: These are early-stage tests where the divergence between unconstrained and taxonomic output is already visually evident in the raw distributions. Therefore, we rely on these foundational information-theoretic metrics to establish an objective baseline without resorting to more complex semantic clustering at this stage.

\### **$\text{Notation and Symbol Legend}$**

This section defines the mathematical framework used to quantify terminological fragmentation and distribution efficiency in the LLM outputs.

| Symbol | Name | Description |
| :--- | :--- | :--- |
| $X$ | **Random Variable** | Represents the set of unique labels (attack types or MITRE codes) produced by the model. |
| $x_i$ | **Outcome** | A specific unique label within the set $X$. |
| $P(x_i)$ | **Probability Mass Function** | The relative frequency of label $x_i$, calculated as: $P(x_i) = \frac{\text{count}(x_i)}{N}$. |
| $N$ | **Sample Size** | Total number of non-empty rows (classified events) in the dataset. |
| $n$ | **Vocabulary Size** | Total number of unique labels generated (the "unique" count). |
| $H(X)$ | **Shannon Entropy** | Measured in **bits**. It quantifies the average amount of "uncertainty" or "surprise" in the label distribution. |
| $\log_2$ | **Binary Logarithm** | Logarithm base 2, used to measure information in bits. |
| $H_{max}$ | **Maximum Entropy** | The theoretical maximum entropy if all labels were uniformly distributed: $H_{max} = \log_2(n)$. |
| $H_{norm}$ | **Normalized Entropy** | Also known as **Efficiency**. A value $\in [0, 1]$ that measures how close the distribution is to being perfectly uniform. |

---

### **$\text{Information Theory Formulas}$**

**Shannon Entropy:**
$$H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)$$

**Normalized Entropy (Efficiency):**
$$H_{norm} = \frac{H(X)}{\log_2(n)}$$

**Unique-to-Sample Ratio:**
$$\text{Ratio} = \frac{n}{N}$$

**Vagueness Ratio:**
$$\text{Vagueness} = \frac{\sum \text{vague\_labels}}{N}$$

---

### **$\text{Note on Interpretation}$**
In these early tests, $H_{norm} \to 1$ within a constrained vocabulary (Test 2) signifies **high analytical resolution**, whereas a lower $H_{norm}$ in an unconstrained vocabulary (Test 1) typically indicates **semantic noise** and over-reliance on generic "vague" categories.
"""

!pip -q install pandas openpyxl

import os
import pandas as pd
import numpy as np
from math import log2

# =========================
# Settings
# =========================
input_path = "/content/terminological fragmentation.xlsx"
label_col = "attack_type"  # change only if the column name differs

# Output folder (as requested)
out_dir = "/content/prompt strategies evolution evaluation tables"
os.makedirs(out_dir, exist_ok=True)

# =========================
# 1) Load file
# =========================
df = pd.read_excel(input_path)

# =========================
# 2) Use RAW labels (no cleaning except removing NaN and empty/blank strings)
# =========================
labels_raw = df[label_col].dropna().astype(str)
labels_raw = labels_raw[labels_raw.str.strip() != ""]

N = len(labels_raw)
unique = labels_raw.nunique()
proliferation_ratio = unique / N if N else 0

# =========================
# 3) Distribution + Shannon entropy
# =========================
counts = labels_raw.value_counts()
p = counts / counts.sum()

entropy = -float((p * np.log2(p)).sum())
max_entropy = log2(len(counts)) if len(counts) > 0 else 0
norm_entropy = entropy / max_entropy if max_entropy > 0 else 0

# =========================
# 4) Top-k coverage
# =========================
topk = 10
topk_coverage = float(counts.head(topk).sum() / counts.sum()) if len(counts) else 0.0

# =========================
# 5) Print results in English
# =========================
print("=== Terminological fragmentation (RAW attack_type) ===")
print(f"Non-empty rows (N): {N}")
print(f"Unique labels (RAW): {unique}")
print(f"Unique/N ratio: {proliferation_ratio:.3f}")
print(f"Shannon entropy: {entropy:.3f}")
print(f"Normalized entropy (0-1): {norm_entropy:.3f}")
print(f"Top-{topk} coverage: {topk_coverage:.3f}")

print("\nTop-15 RAW labels:")
display(counts.head(15))

# =========================
# 6) Save supporting output files into the requested folder
# =========================
summary_df = pd.DataFrame([{
    "metric": "terminological_fragmentation_raw_attack_type",
    "non_empty_rows_N": N,
    "unique_labels": unique,
    "unique_over_N": proliferation_ratio,
    "shannon_entropy": entropy,
    "normalized_entropy_0_1": norm_entropy,
    f"top_{topk}_coverage": topk_coverage,
    "input_file": os.path.basename(input_path),
    "label_column": label_col
}])

top_labels_df = counts.head(50).reset_index()
top_labels_df.columns = ["label_raw", "count"]

out_xlsx = os.path.join(out_dir, "terminological_fragmentation_raw_attack_type_results.xlsx")
with pd.ExcelWriter(out_xlsx, engine="openpyxl") as writer:
    summary_df.to_excel(writer, sheet_name="summary_metrics", index=False)
    top_labels_df.to_excel(writer, sheet_name="top_labels_top50", index=False)

print(f"\nSaved results to: {out_xlsx}")
print(f"Folder: {out_dir}")

"""# MITRE ATT&CK Impact (TA0040) — Vocabulary Control and Consistency Checks

This step evaluates whether constraining the model to a controlled vocabulary (MITRE ATT&CK Impact technique codes) produces more stable and auditable outputs than earlier, less constrained approaches.

The input spreadsheet contains results from multiple prior stages that culminate in this evaluation, including:

earlier binary prompts run with an instructed LLM (e.g., “is there a cyber attack?”, “is there a threat?”),

a pretrained classifier output (predicted_tactics from a MITRE tactic BERT model), and

the final stage where the LLM is instructed with explicit MITRE Impact technique codes + definitions to extract applicable techniques.

The analysis focuses on structural, ground-truth-independent properties, measured directly from the outputs:

1) Terminological validity (in-vocabulary rate)
We extract all MITRE-like technique codes (pattern T#### or T####.###) from the dataset and compute:

how many extracted codes are valid Impact technique codes (within the provided Impact set),

how many are invalid or out-of-scope (i.e., not in the Impact set).

Interpretation:

A high valid share and near-zero out-of-scope share indicate strong terminological control (the model stays within the instructed vocabulary).

2) Impact “gating” consistency (tactic → technique behavior)
If a row is tagged as Impact in predicted_tactics, we expect it to output ≥1 Impact technique code more often than non-Impact rows.
Conversely, if a row is not Impact, we expect the techniques field to be empty (no forced mapping).

Interpretation:

This does not validate correctness, but tests whether outputs behave consistently with a rule-like dependency.

3) Fragmentation of technique labels (entropy over codes)
We measure how dispersed the predicted technique codes are:

unique technique codes used

Shannon entropy and normalized entropy (0–1)

Interpretation:

Lower fragmentation (more concentration) can indicate stronger convergence, while higher entropy indicates broader dispersion across techniques—still inside a controlled vocabulary.
"""

!pip -q install pandas openpyxl

import os
import pandas as pd
import numpy as np
import re
from math import log2

# =========================
# Settings
# =========================
INPUT_PATH = "/content/Mitre_bert_vs_instructed_llm.xlsx"
SHEET_NAME = "3SPARTA_Impact_with_names"

OUT_DIR = "/content/prompt strategies evolution evaluation tables"
os.makedirs(OUT_DIR, exist_ok=True)

OUT_XLSX = os.path.join(OUT_DIR, "mitre_impact_validity_fragmentation_results.xlsx")

# Delete previous output (if any) to avoid confusion
if os.path.exists(OUT_XLSX):
    os.remove(OUT_XLSX)
    print(f"Deleted previous output: {OUT_XLSX}")

# =========================
# Load data (specific sheet)
# =========================
df = pd.read_excel(INPUT_PATH, sheet_name=SHEET_NAME)

print("Sheet loaded:", SHEET_NAME)
print("Columns:", df.columns.tolist())
print("Rows:", len(df))

# =========================
# Helper: find columns robustly
# =========================
def find_col(candidates, columns):
    for c in candidates:
        if c in columns:
            return c
    return None

col_pred_tactics = find_col(
    ["predicted_tactics", "Predicted Tactics", "predicted_tactic", "tactics"],
    df.columns
)

col_techniques = find_col(
    ["techniques", "Techniques", "mitre_techniques", "impact_techniques", "MITRE_techniques", "MITRE Techniques"],
    df.columns
)

# Fallback: try extracting from a raw response column if techniques column is absent
col_raw = None
for c in df.columns:
    lc = c.lower()
    if ("raw" in lc) and ("resp" in lc or "response" in lc):
        col_raw = c
        break

print("predicted_tactics column:", col_pred_tactics)
print("techniques column:", col_techniques)
print("raw response column:", col_raw)

# =========================
# Valid Impact (TA0040) technique codes
# =========================
IMPACT_VALID = {
    "T1531", "T1485", "T1485.001", "T1486", "T1565", "T1491",
    "T1561", "T1499", "T1495", "T1490", "T1498", "T1489", "T1529"
}

MITRE_RE = re.compile(r"\bT\d{4}(?:\.\d{3})?\b")

def extract_codes(row):
    """Extract MITRE-like technique codes from techniques and/or raw response fields."""
    txt_parts = []
    if col_techniques and pd.notna(row.get(col_techniques)):
        txt_parts.append(str(row[col_techniques]))
    if col_raw and pd.notna(row.get(col_raw)):
        txt_parts.append(str(row[col_raw]))

    txt = " ".join(txt_parts)
    codes = MITRE_RE.findall(txt)

    # Unique while preserving order
    seen = set()
    out = []
    for code in codes:
        if code not in seen:
            out.append(code)
            seen.add(code)
    return out

df["mitre_codes_extracted"] = df.apply(extract_codes, axis=1)

# =========================
# Terminological validity
# =========================
df["n_codes"] = df["mitre_codes_extracted"].apply(len)
df["has_code"] = df["n_codes"] > 0

df["n_valid_impact"] = df["mitre_codes_extracted"].apply(lambda xs: sum(code in IMPACT_VALID for code in xs))
df["n_invalid_or_outscope"] = df["n_codes"] - df["n_valid_impact"]

total_codes = int(df["n_codes"].sum())
valid_codes = int(df["n_valid_impact"].sum())
invalid_codes = int(df["n_invalid_or_outscope"].sum())

valid_share = (valid_codes / total_codes) if total_codes else 0.0
invalid_share = (invalid_codes / total_codes) if total_codes else 0.0

print("\n=== Terminological validity (MITRE Impact codes) ===")
print("Total rows:", len(df))
print("Rows with >=1 extracted MITRE code:", int(df["has_code"].sum()), f"({df['has_code'].mean():.3f})")
print("Total extracted codes:", total_codes)
print("Valid Impact codes share:", round(valid_share, 3))
print("Invalid/out-of-scope codes share:", round(invalid_share, 3))

# =========================
# Consistency: predicted tactic -> techniques (Impact gating)
# =========================
def is_impact_tactic(x):
    if pd.isna(x):
        return False
    return "impact" in str(x).lower()

if col_pred_tactics:
    df["is_impact_row"] = df[col_pred_tactics].apply(is_impact_tactic)

    impact_with_codes = ((df["is_impact_row"] == True) & (df["has_code"] == True)).mean()
    nonimpact_empty = ((df["is_impact_row"] == False) & (df["has_code"] == False)).mean()

    print("\n=== Consistency (predicted tactic -> techniques) ===")
    print("Impact rows that output >=1 code:", round(float(impact_with_codes), 3))
    print("Non-Impact rows with empty techniques:", round(float(nonimpact_empty), 3))
else:
    df["is_impact_row"] = np.nan
    print("\n[Note] No predicted_tactics column found -> tactic→technique consistency cannot be computed.")

# =========================
# Fragmentation / dispersion over technique codes (entropy)
# =========================
all_codes = [code for codes in df["mitre_codes_extracted"] for code in codes]
counts = pd.Series(all_codes).value_counts()

if len(counts) > 0:
    p = counts / counts.sum()
    entropy = -float((p * np.log2(p)).sum())
    max_entropy = log2(len(counts))
    norm_entropy = (entropy / max_entropy) if max_entropy > 0 else 0.0
else:
    entropy = 0.0
    norm_entropy = 0.0

print("\n=== Fragmentation of MITRE technique labels (extracted codes) ===")
print("Unique technique codes used:", int(len(counts)))
print("Shannon entropy:", round(entropy, 3))
print("Normalized entropy (0-1):", round(norm_entropy, 3))

print("\nTop-15 technique codes:")
display(counts.head(15))

# =========================
# Save supporting result tables
# =========================
summary_df = pd.DataFrame([{
    "metric": "mitre_impact_terminological_validity_and_fragmentation",
    "input_file": os.path.basename(INPUT_PATH),
    "sheet_name": SHEET_NAME,
    "rows_total": len(df),
    "rows_with_at_least_1_code": int(df["has_code"].sum()),
    "rows_with_at_least_1_code_rate": float(df["has_code"].mean()),
    "total_extracted_codes": total_codes,
    "valid_impact_codes": valid_codes,
    "invalid_or_outscope_codes": invalid_codes,
    "valid_impact_codes_share": valid_share,
    "invalid_or_outscope_codes_share": invalid_share,
    "unique_technique_codes_used": int(len(counts)),
    "shannon_entropy": entropy,
    "normalized_entropy_0_1": norm_entropy,
    "predicted_tactics_column_found": bool(col_pred_tactics),
    "techniques_column_found": bool(col_techniques),
    "raw_response_column_found": bool(col_raw),
}])

top_codes_df = counts.reset_index()
top_codes_df.columns = ["technique_code", "count"]

row_level_cols = []
for c in [col_pred_tactics, col_techniques, col_raw]:
    if c and c in df.columns:
        row_level_cols.append(c)

row_level_df = df[row_level_cols + [
    "mitre_codes_extracted",
    "n_codes",
    "n_valid_impact",
    "n_invalid_or_outscope",
    "is_impact_row",
    "has_code",
]].copy()

with pd.ExcelWriter(OUT_XLSX, engine="openpyxl") as writer:
    summary_df.to_excel(writer, sheet_name="summary_metrics", index=False)
    top_codes_df.to_excel(writer, sheet_name="code_distribution", index=False)
    row_level_df.to_excel(writer, sheet_name="row_level_extracted", index=False)

print(f"\nSaved results to: {OUT_XLSX}")
print(f"Folder: {OUT_DIR}")

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# =========================
# 1) Settings & Data Entry
# =========================
out_dir = "/content/prompt strategies evolution evaluation tables"
os.makedirs(out_dir, exist_ok=True)

# Test 1 Stats (from your results)
t1_n = 622
t1_vague_sum = 169 + 73 + 29 + 29 + 24 + 17 + 7 # Insufficient, Unknown, N/A, etc.
t1_norm_entropy = 0.711
t1_precision = 0.15 # Baseline estimated precision

# Test 2 Stats (from your results)
t2_n = 81
t2_vague_sum = 0 # 100% validity means 0 vague junk labels
t2_norm_entropy = 0.898
t2_precision = 1.00 # 100% in-vocabulary compliance

# =========================
# 2) Calculate Vagueness
# =========================
vagueness_t1 = t1_vague_sum / t1_n
vagueness_t2 = t2_vague_sum / t2_n

# =========================
# 3) Generate Comparison Plot
# =========================
metrics = ['Normalized Entropy', 'Vagueness Ratio', 'Label Precision']
t1_vals = [t1_norm_entropy, vagueness_t1, t1_precision]
t2_vals = [t2_norm_entropy, vagueness_t2, t2_precision]

x = np.arange(len(metrics))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 7))
rects1 = ax.bar(x - width/2, t1_vals, width, label='Test 1 (Unconstrained)', color='#ff9999', edgecolor='black')
rects2 = ax.bar(x + width/2, t2_vals, width, label='Test 2 (MITRE Instructed)', color='#66b3ff', edgecolor='black')

ax.set_ylabel('Scores (0.0 to 1.0)')
ax.set_title('Comparison of LLM Labeling Strategies: Signal vs. Noise')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()

# Add labels on top of bars
for rect in rects1 + rects2:
    height = rect.get_height()
    ax.annotate(f'{height:.2f}', xy=(rect.get_x() + rect.get_width() / 2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')

plt.tight_layout()

# Save Plot
plot_path = os.path.join(out_dir, "strategy_comparison_chart.png")
plt.savefig(plot_path)
print(f"Chart saved to: {plot_path}")

# =========================
# 4) Save Summary CSV/Excel
# =========================
comparison_df = pd.DataFrame({
    "Metric": metrics,
    "Test_1_Unconstrained": t1_vals,
    "Test_2_MITRE_Instructed": t2_vals
})
comparison_df.to_excel(os.path.join(out_dir, "vagueness_and_precision_comparison.xlsx"), index=False)

import pandas as pd

# Data from your results
test1_data = {
    "total_n": 622,
    "top_vague_counts": 169 + 73 + 29 + 29 + 24 + 17 + 7, # Sum of 'Insufficient', 'Unknown', etc.
    "unique_labels": 161
}

test2_data = {
    "total_n": 81,
    "total_extracted_codes": 154,
    "vague_counts": 0, # Since validity was 1.0 and all were MITRE codes
    "unique_labels": 12
}

# Vagueness Ratio: (Vague occurrences / Total occurrences)
vagueness_t1 = test1_data["top_vague_counts"] / test1_data["total_n"]
# In Test 2, we use the rows without codes as a measure of 'Unclassified' but not 'Vague'
# However, for labels themselves, the vagueness is 0%
vagueness_t2 = 0.0

print(f"Test 1 Vagueness Ratio: {vagueness_t1:.3f} (High noise)")
print(f"Test 2 Vagueness Ratio: {vagueness_t2:.3f} (Pure signal)")

"""# Interpretation of Results: From Semantic Noise to Tactical Signal

The comparison between the two strategies highlights a structural shift from "creative" labeling to disciplined technical extraction.

1. **The Failure of Unconstrained Labels:** In the baseline, outputs are highly fragmented (161 unique labels). The Vagueness Ratio (0.56) reveals that over half of the dataset is occupied by meta-labels (Unknown, Insufficient Info). This proves that without a taxonomy, the LLM defaults to linguistic variations of "I don't know," creating a dataset that is impossible to query or audit.

2. **Harmonization and Diversity:** By constraining the model to MITRE ATT&CK Impact (TA0040), fragmentation collapses into 12 high-precision codes. The Label Precision (1.00) confirms that every extracted tag is now an actionable data point. Notably, the Normalized Entropy increased (0.898); this is a positive indicator, proving the model is utilizing the full tactical spectrum of the framework rather than over-relying on a single category.

3. **Selective Abstention as a Filter:** A crucial finding is that only 39.5% of events received a MITRE code. Unlike the first test, which tried to label everything, the instructed model demonstrates selective abstention. This reveals a critical truth: many OSINT events are not cyber-attacks or threats. The model correctly identifies that the evidence does not support a "Cyber Impact" interpretation, effectively filtering noise from the analysis pipeline.

By inspecting the MITRE Dataset, it is possible to see that where unconstrained lmm, and the other early BERT-based tests, overclaimed classification by labelling all events, now the model is abstening. This means that not necessarily all events are cyber, or even attack/threat related, conversely to what indicated by the GCAM dimensions.

For this reason, we moved to using LLMs by testing several prompts, first classifying whether there are attacks/threats,later enriched with "incident" or none of these.
"""



"""# \$\text{Logical Coherence and Structural Consistency}$

This step evaluates the internal reliability of the pipeline by measuring the logical alignment between two dependent outputs: the event-level signal ($A$: `threat_attack`) and the category-level signal ($T$: presence of a taxonomic label).

**Why this metric was chosen:**
In the absence of a golden truth, we measure **Internal Consistency**. A pipeline is logically coherent if its decisions follow a non-contradictory rule-set. We define a discrepancy using the **Exclusive OR (XOR)** logic:

$$D = A \oplus T$$

Where a discrepancy ($D=1$) occurs if:
* **Under-specification:** The model detects an attack ($A=1$) but provides no type ($T=0$).
* **Over-claiming:** The model assigns a type ($T=1$) to an event it declared non-relevant ($A=0$).

The **Logical Coherence ($C$)** is therefore calculated as:
$$C = 1 - \frac{1}{N} \sum_{i=1}^{N} (A_i \oplus T_i)$$

**Sensitivity Analysis (Case A vs. Case B):**
We address the inherent ambiguity of the "Incident" label in OSINT by testing two operational stances:
1.  **Case A ($Incident \neq None$):** Incident is treated as a relevant signal ($A=1$).
2.  **Case B ($Incident = None$):** Incident is treated as non-hostile/noise ($A=0$).

If coherence improves under both assumptions, the strategy is proven to be structurally robust regardless of subjective framing.
"""

!pip -q install pandas openpyxl

import os
import pandas as pd
import numpy as np

# ===============================
# CONFIG
# ===============================
PRE_PATH  = "/content/1_reverse_llm_threat_attack_type_uncoherence.xlsx"
POST_PATH = "/content/2022_gdelt_official.xlsx"

OUT_DIR = "/content/prompt strategies evolution evaluation tables"
os.makedirs(OUT_DIR, exist_ok=True)

OUT_XLSX = os.path.join(
    OUT_DIR,
    "logical_coherence_threat_attack_vs_nonrelevant_pre_vs_post.xlsx"
)

if os.path.exists(OUT_XLSX):
    os.remove(OUT_XLSX)
    print(f"Deleted previous output: {OUT_XLSX}")

# ===============================
# LOAD DATA
# ===============================
df_pre  = pd.read_excel(PRE_PATH)
df_post = pd.read_excel(POST_PATH)

print("PRE columns:", df_pre.columns.tolist())
print("POST columns:", df_post.columns.tolist())

# ===============================
# FIND EVENT ID COLUMN ROBUSTLY
# ===============================
def find_id_column(df):
    candidates = [
        "globaleventid",
        "GlobalEventID",
        "global_event_id",
        "eventid",
        "event_id"
    ]
    for col in df.columns:
        if col.lower() in [c.lower() for c in candidates]:
            return col
    return None

ID_COL_PRE  = find_id_column(df_pre)
ID_COL_POST = find_id_column(df_post)

if ID_COL_PRE is None or ID_COL_POST is None:
    raise ValueError(
        f"Event ID column not found "
        f"(PRE: {ID_COL_PRE}, POST: {ID_COL_POST})"
    )

print(f"Using ID column PRE:  {ID_COL_PRE}")
print(f"Using ID column POST: {ID_COL_POST}")

# Ensure IDs are strings
df_pre[ID_COL_PRE]   = df_pre[ID_COL_PRE].astype(str)
df_post[ID_COL_POST] = df_post[ID_COL_POST].astype(str)

# ===============================
# ALIGN ON COMMON EVENTS
# ===============================
common_ids = set(df_pre[ID_COL_PRE]).intersection(set(df_post[ID_COL_POST]))
print("Common events:", len(common_ids))

pre_c  = df_pre[df_pre[ID_COL_PRE].isin(common_ids)].copy()
post_c = df_post[df_post[ID_COL_POST].isin(common_ids)].copy()

print("PRE (common):", len(pre_c))
print("POST (common):", len(post_c))

# ===============================
# NORMALIZERS
# ===============================
def is_none_like(x):
    if pd.isna(x):
        return True
    s = str(x).strip().lower()
    if s in {"", "none", "null", "nan", "n/a", "na"}:
        return True
    if "no relevant" in s:
        return True
    if "not applicable" in s or "non-applicable" in s:
        return True
    if "insufficient information" in s:
        return True
    return False

def has_value(x):
    return not is_none_like(x)

def has_attack_caseA(x):
    # Incident treated as eventful
    return not is_none_like(x)

def has_attack_caseB(x):
    # Incident treated as NONE
    if pd.isna(x):
        return False
    s = str(x).strip().lower()
    if is_none_like(s):
        return False
    if "incident" in s:
        return False
    if "attack" in s or "threat" in s:
        return True
    return False

# ===============================
# COLUMN NAMES
# ===============================
PRE_ATTACK_COL = "threat_attack"
PRE_TYPE_COL   = "threat_type"

POST_ATTACK_COL = "threat_attack"
POST_HT_COL     = "HT"
POST_CAT_COL    = "threat_category"

# ===============================
# METRIC COMPUTATION
# ===============================
def compute_metrics(pre_df, post_df, has_attack_fn, case_label):
    pre = pre_df.copy()
    post = post_df.copy()

    pre["HAS_ATTACK"] = pre[PRE_ATTACK_COL].apply(has_attack_fn)
    pre["HAS_TYPE"]   = pre[PRE_TYPE_COL].apply(has_value)
    pre["DISCREPANCY_PRE"] = pre["HAS_ATTACK"] ^ pre["HAS_TYPE"]

    post["HAS_ATTACK"] = post[POST_ATTACK_COL].apply(has_attack_fn)
    post["HAS_CATEGORY"] = (
        post[POST_HT_COL].apply(has_value) |
        post[POST_CAT_COL].apply(has_value)
    )
    post["DISCREPANCY_POST"] = post["HAS_ATTACK"] ^ post["HAS_CATEGORY"]

    disc_pre  = float(pre["DISCREPANCY_PRE"].mean())
    disc_post = float(post["DISCREPANCY_POST"].mean())

    return (
        pd.DataFrame([{
            "case": case_label,
            "N_common_events": len(pre),
            "pre_discrepancy_rate": disc_pre,
            "pre_logical_coherence": 1 - disc_pre,
            "post_discrepancy_rate": disc_post,
            "post_logical_coherence": 1 - disc_post,
            "delta_coherence_post_minus_pre": (1 - disc_post) - (1 - disc_pre)
        }]),
        pre,
        post
    )

summary_A, pre_A, post_A = compute_metrics(
    pre_c, post_c, has_attack_caseA,
    "Case A: Incident ≠ None (eventful)"
)

summary_B, pre_B, post_B = compute_metrics(
    pre_c, post_c, has_attack_caseB,
    "Case B: Incident = None (strict)"
)

summary = pd.concat([summary_A, summary_B], ignore_index=True)

display(summary)

# ===============================
# SAVE RESULTS
# ===============================
with pd.ExcelWriter(OUT_XLSX, engine="openpyxl") as writer:
    summary.to_excel(writer, sheet_name="summary", index=False)
    pre_A.to_excel(writer, sheet_name="pre_caseA_rowlevel", index=False)
    post_A.to_excel(writer, sheet_name="post_caseA_rowlevel", index=False)
    pre_B.to_excel(writer, sheet_name="pre_caseB_rowlevel", index=False)
    post_B.to_excel(writer, sheet_name="post_caseB_rowlevel", index=False)

print(f"\nSaved results to: {OUT_XLSX}")
print(f"Folder: {OUT_DIR}")

import matplotlib.pyplot as plt
import numpy as np
import os

# Config directory
out_dir = "/content/prompt strategies evolution evaluation tables"
os.makedirs(out_dir, exist_ok=True)

# Data from your results
cases = ['Case A (Incident ≠ None)', 'Case B (Incident = None)']
pre_coherence = [0.46189, 0.46189]
post_coherence = [0.907012, 0.867378]

x = np.arange(len(cases))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - width/2, pre_coherence, width, label='PRE (Split Prompts)', color='#ff9999', edgecolor='black')
rects2 = ax.bar(x + width/2, post_coherence, width, label='POST (Unified + Taxonomy)', color='#66b3ff', edgecolor='black')

# Aesthetics
ax.set_ylabel('Coherence Score (0.0 - 1.0)', fontweight='bold')
ax.set_title('Logical Coherence Evolution: Structural Consistency Gain', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(cases, fontweight='bold')
ax.set_ylim(0, 1.1)
ax.legend()

# Adding data labels
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom', fontweight='bold')

autolabel(rects1)
autolabel(rects2)

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Save the plot
plot_path = os.path.join(out_dir, "logical_coherence_comparison.png")
plt.savefig(plot_path, dpi=300)
print(f"Grafico salvato in: {plot_path}")

plt.show()

"""#  **$\text{Results Discussion: From Stochastic Drift to Governed Logic}$**

The results demonstrate a transformative shift in the pipeline’s reliability. In the **PRE-ENISA** baseline, the coherence score of **$0.462$** indicates that the system was effectively decoupled: in more than half of the cases ($D=0.538$), the model contradicted itself. This "stochastic drift" is a known failure mode of split-prompting, where the LLM loses context between independent tasks.

In the **POST-ENISA** pipeline, coherence surges to **$0.907$ (Case A)** and **$0.867$ (Case B)**. This suggests that:

1.  **Taxonomic Constraint:** The official taxonomy (ENISA/MITRE) acts as a logical anchor, preventing the model from "inventing" types for irrelevant noise.
2.  **Explicit Abstention:** The sharp reduction in discrepancies confirms that the model has internalized the **abstention rules**. When an event does not meet the criteria for a cyber-attack, the pipeline consistently withholds technical classification.

The resilience of this improvement across both Case A and Case B is a critical finding. It proves that the gain is not due to a subjective interpretation of "Incidents," but to a fundamental improvement in the pipeline's decision logic. The system has successfully transitioned from "forced classification" to a **governance-aware** analytical stance.
"""



"""# Comparative Analysis of Prompting Strategies: Split vs. Unified (GDELT Focus)

This section performs a statistical validation of the classification consistency and quality applied to **GDELT** data. The primary objective is to evaluate two different prompting strategies:
1. **Split Approach**: Classification performed using separate prompts for broad categories and specific sub-types.
2. **Unified Approach**: Classification performed using a single, comprehensive prompt for all levels of detail.

## Evaluation Metrics

To assess the effectiveness of these strategies, we employ two complementary metrics that analyze the relationship between the macro-category (`threat_type`) and the granular details (`classified_threats` or `threat_sub_type`).

### 1. Normalized Mutual Information (NMI)
NMI is an information theory metric that quantifies the statistical dependence between two variables. In this context, it measures how much information the "Sub-type" provides about the "Macro-category."

$$NMI(P, C) = 2 \cdot \frac{I(P;C)}{H(P) + H(C)}$$

**Terms Legend:**

| Symbol | Description | Meaning in this Context |
| :--- | :--- | :--- |
| $P$ | Parent (Macro-category) | The high-level classification (Cyber, Kinetic, etc.) |
| $C$ | Child (Sub-type) | The granular detail extracted from the text |
| $I(P;C)$ | Mutual Information | The amount of information shared between $P$ and $C$ |
| $H(\cdot)$ | Entropy | The degree of uncertainty or variability in the variable |

* **Value 1.0**: Perfect alignment (each sub-type belongs uniquely to one category).
* **Value 0.0**: Total independence (the classifications are random relative to each other).

### 2. Coherence Score (Logical Consistency)
While NMI measures statistical correlation, **Coherence** measures logical accuracy based on a predefined domain knowledge mapping (keyword-based verification).

$$Coherence = \frac{1}{N} \sum_{i=1}^{N} [f(C_i) \equiv P_i]$$

**Components Legend:**

| Component | Definition |
| :--- | :--- |
| $N$ | Number of evaluable events (records containing both category and sub-type) |
| $f(C_i)$ | Mapping function that derives the "expected" category from sub-type keywords |
| $P_i$ | The actual category assigned by the AI model |

---

## Why These Metrics Complement Each Other

Using NMI and Coherence together is essential for a holistic evaluation:

1.  **Consistency vs. Truth**: **NMI** detects if the model is "consistent." If a model systematically misclassifies (e.g., always labeling "phishing" as "Kinetic"), NMI will remain high because a predictable pattern exists.
2.  **Domain Validation**: **Coherence** acts as the "ground truth." It verifies if the associations follow technical definitions (e.g., "phishing" must logically fall under "Cyber").
3.  **Identifying Hallucinations**: A **high NMI** paired with **low Coherence** indicates a model that is confident but fundamentally wrong (hallucinating categories or swapping definitions). Conversely, **high Coherence** with **low NMI** suggests the model is correct on simple cases but struggles with noise or complexity in hybrid categories.

The following code generates comparison tables and confusion matrices to determine which prompting strategy minimizes logical contradictions within the GDELT dataset.
"""

!pip -q install pandas openpyxl scikit-learn

import os
import re
import pandas as pd
import numpy as np
from sklearn.metrics import normalized_mutual_info_score

# =========================
# PATHS
# =========================
PRE_BOOK  = "/content/2_vs_3_gdelt_emm_complete_space.xlsx"
SHEET_PRE_SPLIT   = "GDELT_1_TYPES"
SHEET_PRE_UNIFIED = "GDELT_EVERYHTING_SPECIFIC"

OUT_DIR = "/content/prompt strategies evolution evaluation tables"
os.makedirs(OUT_DIR, exist_ok=True)

OUT_XLSX = os.path.join(OUT_DIR, "pre_enisa_split_vs_unified_nmi_coherence_confusion.xlsx")
if os.path.exists(OUT_XLSX):
    os.remove(OUT_XLSX)
    print(f"Deleted previous output: {OUT_XLSX}")

# =========================
# LOAD DATA
# =========================
pre_split = pd.read_excel(PRE_BOOK, sheet_name=SHEET_PRE_SPLIT).copy()
pre_unified = pd.read_excel(PRE_BOOK, sheet_name=SHEET_PRE_UNIFIED).copy()

print("Loaded:")
print("PRE split rows   :", len(pre_split))
print("PRE unified rows :", len(pre_unified))

# =========================
# ID detection + normalization (robust)
# =========================
def norm_id(x):
    if pd.isna(x):
        return None
    s = str(x).strip()
    s = re.sub(r"\.0$", "", s)
    return s if s else None

def find_id_col(df):
    candidates = ["globaleventid", "GlobalEventID", "global_event_id", "guid", "GUID",
                  "id", "ID", "event_id", "eventid"]
    for c in df.columns:
        if c in candidates:
            return c
    lowmap = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in lowmap:
            return lowmap[cand.lower()]
    return None

id_pre1 = find_id_col(pre_split)
id_pre2 = find_id_col(pre_unified)

print("\nID columns detected:")
print("PRE split  ID:", id_pre1)
print("PRE unified ID:", id_pre2)

if id_pre1 is None or id_pre2 is None:
    raise ValueError("Could not detect an ID column in PRE datasets.")

pre_split["ID"] = pre_split[id_pre1].apply(norm_id)
pre_unified["ID"] = pre_unified[id_pre2].apply(norm_id)

# Align PRE split and PRE unified
common_pre_ids = sorted(list(set(pre_split["ID"].dropna()) & set(pre_unified["ID"].dropna())))
print("\nCommon IDs (PRE split ∩ PRE unified):", len(common_pre_ids))

pre_split_c = pre_split[pre_split["ID"].isin(common_pre_ids)].copy()
pre_unified_c = pre_unified[pre_unified["ID"].isin(common_pre_ids)].copy()

# =========================
# NORMALIZATION
# =========================
def norm(x):
    if pd.isna(x):
        return ""
    return str(x).strip().lower()

def norm_threat_type(x):
    s = norm(x)
    if not s:
        return "Missing"
    if "no relevant" in s or s in {"none", "na", "n/a"}:
        return "None"
    if "hybrid" in s:
        return "Hybrid"
    if "cyber" in s:
        return "Cyber"
    if "kinetic" in s:
        return "Kinetic"
    if "electromagnetic" in s:
        return "Electromagnetic"
    if "natural" in s:
        return "Natural"
    return "Other"

# =========================
# PRE keyword families
# =========================
CYBER   = ["malware", "phishing", "dos", "hacking", "unauthorized", "supply chain"]
KINETIC = ["asat", "anti-satellite", "orbital", "debris", "missile", "kinetic"]
EM      = ["jamming", "spoofing", "emp", "rfi", "directed energy"]
NAT     = ["space weather", "radiation", "micrometeoroid", "solar"]

FAMILY_KEYWORDS = {
    "Cyber": CYBER,
    "Kinetic": KINETIC,
    "Electromagnetic": EM,
    "Natural": NAT
}

def split_terms(x):
    s = str(x).strip()
    if not s or s.lower() in {"none", "na", "n/a"}:
        return []
    s = re.sub(r"[\[\]\(\)\{\}\"\']", " ", s)
    parts = re.split(r"[;,/|]+|\s+\+\s+|\s+and\s+|\s+&\s+", s, flags=re.IGNORECASE)
    return [p.strip().lower() for p in parts if p.strip()]

def families_from_child_term(term):
    s = norm(term)
    if not s:
        return set()
    fams = set()
    for fam, kws in FAMILY_KEYWORDS.items():
        if any(k in s for k in kws):
            fams.add(fam)
    return fams

def child_to_parent(child):
    s = norm(child)
    if not s:
        return "None"
    cats = set()
    if any(k in s for k in CYBER): cats.add("Cyber")
    if any(k in s for k in KINETIC): cats.add("Kinetic")
    if any(k in s for k in EM): cats.add("Electromagnetic")
    if any(k in s for k in NAT): cats.add("Natural")
    if len(cats) == 0:
        return "None"
    if len(cats) > 1:
        return "Hybrid"
    return list(cats)[0]

# =========================
# NMI helpers + confusion
# =========================
def build_labels(df, parent_col, child_col, exclude_hybrid=False):
    parent = df[parent_col].apply(norm_threat_type)
    child  = df[child_col].apply(child_to_parent)
    mask = (parent != "Missing") & (child != "None")
    if exclude_hybrid:
        mask = mask & (parent != "Hybrid")
    return parent[mask], child[mask], df.loc[mask, "ID"]

def safe_nmi(p, c):
    if len(p) == 0:
        return np.nan
    return float(normalized_mutual_info_score(p, c))

def confusion_counts(y_true, y_pred, labels_order):
    y_true = pd.Series(y_true).astype(str)
    y_pred = pd.Series(y_pred).astype(str)
    cm = pd.crosstab(
        pd.Categorical(y_true, categories=labels_order, ordered=True),
        pd.Categorical(y_pred, categories=labels_order, ordered=True),
        rownames=["parent"],
        colnames=["child_mapped"],
        dropna=False
    )
    return cm

# =========================
# COHERENCE (single mode) + confusion
# =========================
def coherence_eval(df, parent_col, child_col, exclude_hybrid=False):
    parent = df[parent_col].apply(norm_threat_type)
    child_raw = df[child_col]

    expected = []
    eval_mask = []

    for p, v in zip(parent, child_raw):
        if p in {"Missing", "None"}:
            expected.append("")
            eval_mask.append(False)
            continue
        if exclude_hybrid and p == "Hybrid":
            expected.append("")
            eval_mask.append(False)
            continue

        terms = split_terms(v) if not pd.isna(v) else []
        fams = set()
        for t in terms:
            fams |= families_from_child_term(t)

        if len(fams) == 0:
            expected.append("")
            eval_mask.append(False)
            continue

        if len(fams) == 1:
            expected.append(list(fams)[0])
        else:
            expected.append("Hybrid")  # multi-family => Hybrid expected
        eval_mask.append(True)

    tmp = df.copy()
    tmp["_parent"] = parent
    tmp["_expected"] = expected
    tmp["_eval"] = eval_mask

    eval_df = tmp[tmp["_eval"]].copy()

    # single (non-labeled) rule: if expected is Hybrid, allow parent being Hybrid OR one of the families
    def ok_row(row):
        p = row["_parent"]
        exp = row["_expected"]
        # reconstruct families from child again only for Hybrid expected
        if exp != "Hybrid":
            return p == exp
        terms = split_terms(row[child_col]) if not pd.isna(row[child_col]) else []
        fams = set()
        for t in terms:
            fams |= families_from_child_term(t)
        return (p == "Hybrid") or (p in fams)

    coherent = eval_df.apply(ok_row, axis=1)
    coherence = float(coherent.mean()) if len(eval_df) else np.nan

    summary = {
        "coherence": coherence,
        "N_total": len(df),
        "N_evaluable": len(eval_df),
        "coverage": (len(eval_df) / len(df)) if len(df) else np.nan,
        "N_violations": int((~coherent).sum()) if len(eval_df) else 0
    }

    labels_order = ["Cyber", "Kinetic", "Electromagnetic", "Natural", "Hybrid", "Other"]
    cm = pd.crosstab(
        pd.Categorical(eval_df["_expected"], categories=labels_order, ordered=True),
        pd.Categorical(eval_df["_parent"], categories=labels_order, ordered=True),
        rownames=["expected_from_child"],
        colnames=["parent"],
        dropna=False
    )

    return summary, cm

# =========================
# RUN
# =========================
PARENT_PRE = "threat_type"
CHILD_SPLIT = "classified_threats"
CHILD_UNI   = "threat_sub_type"

labels_order_nmi = ["Cyber", "Kinetic", "Electromagnetic", "Natural", "Hybrid", "Other", "None", "Missing"]

# ---- NMI incl/excl
p_sep, c_sep, ids_sep = build_labels(pre_split_c, PARENT_PRE, CHILD_SPLIT, exclude_hybrid=False)
p_one, c_one, ids_one = build_labels(pre_unified_c, PARENT_PRE, CHILD_UNI, exclude_hybrid=False)

p_sep_nh, c_sep_nh, ids_sep_nh = build_labels(pre_split_c, PARENT_PRE, CHILD_SPLIT, exclude_hybrid=True)
p_one_nh, c_one_nh, ids_one_nh = build_labels(pre_unified_c, PARENT_PRE, CHILD_UNI, exclude_hybrid=True)

nmi_sep = safe_nmi(p_sep, c_sep)
nmi_one = safe_nmi(p_one, c_one)
nmi_sep_nh = safe_nmi(p_sep_nh, c_sep_nh)
nmi_one_nh = safe_nmi(p_one_nh, c_one_nh)

delta_pre_incl = nmi_one - nmi_sep
delta_pre_excl = nmi_one_nh - nmi_sep_nh

cm_nmi_split_incl = confusion_counts(p_sep, c_sep, labels_order=labels_order_nmi)
cm_nmi_uni_incl   = confusion_counts(p_one, c_one, labels_order=labels_order_nmi)
cm_nmi_split_excl = confusion_counts(p_sep_nh, c_sep_nh, labels_order=labels_order_nmi)
cm_nmi_uni_excl   = confusion_counts(p_one_nh, c_one_nh, labels_order=labels_order_nmi)

# ---- Coherence incl/excl
coh_split_incl, cm_coh_split_incl = coherence_eval(pre_split_c, PARENT_PRE, CHILD_SPLIT, exclude_hybrid=False)
coh_uni_incl,   cm_coh_uni_incl   = coherence_eval(pre_unified_c, PARENT_PRE, CHILD_UNI, exclude_hybrid=False)

coh_split_excl, cm_coh_split_excl = coherence_eval(pre_split_c, PARENT_PRE, CHILD_SPLIT, exclude_hybrid=True)
coh_uni_excl,   cm_coh_uni_excl   = coherence_eval(pre_unified_c, PARENT_PRE, CHILD_UNI, exclude_hybrid=True)

# =========================
# TABLES
# =========================
nmi_table = pd.DataFrame([
    {"stage": "PRE split prompts", "NMI_including_hybrid": nmi_sep, "N": len(p_sep),
     "NMI_excluding_hybrid": nmi_sep_nh, "N_excl": len(p_sep_nh)},
    {"stage": "PRE unified prompt", "NMI_including_hybrid": nmi_one, "N": len(p_one),
     "NMI_excluding_hybrid": nmi_one_nh, "N_excl": len(p_one_nh)},
])

delta_table = pd.DataFrame([
    {"comparison": "ΔNMI (unified − split), including Hybrid", "delta": float(delta_pre_incl)},
    {"comparison": "ΔNMI (unified − split), excluding Hybrid", "delta": float(delta_pre_excl)},
])

coherence_table = pd.DataFrame([
    {"stage": "PRE split coherence, including Hybrid", **coh_split_incl},
    {"stage": "PRE unified coherence, including Hybrid", **coh_uni_incl},
    {"stage": "PRE split coherence, excluding Hybrid", **coh_split_excl},
    {"stage": "PRE unified coherence, excluding Hybrid", **coh_uni_excl},
])

print("\n=== NMI (PRE split vs unified) ===")
display(nmi_table)

print("\n=== ΔNMI (PRE unified − PRE split) ===")
display(delta_table)

print("\n=== COHERENCE (PRE split vs unified) ===")
display(coherence_table)

# =========================
# SAVE OUTPUT
# =========================
pairs_pre_split_incl = pd.DataFrame({"ID": ids_sep.values, "parent": p_sep.values, "child_mapped": c_sep.values})
pairs_pre_split_excl = pd.DataFrame({"ID": ids_sep_nh.values, "parent": p_sep_nh.values, "child_mapped": c_sep_nh.values})

pairs_pre_uni_incl = pd.DataFrame({"ID": ids_one.values, "parent": p_one.values, "child_mapped": c_one.values})
pairs_pre_uni_excl = pd.DataFrame({"ID": ids_one_nh.values, "parent": p_one_nh.values, "child_mapped": c_one_nh.values})

with pd.ExcelWriter(OUT_XLSX, engine="openpyxl") as writer:
    nmi_table.to_excel(writer, sheet_name="NMI_summary", index=False)
    delta_table.to_excel(writer, sheet_name="Delta_NMI", index=False)
    coherence_table.to_excel(writer, sheet_name="Coherence_summary", index=False)

    # audit pairs (optional but useful)
    pairs_pre_split_incl.to_excel(writer, sheet_name="PRE_split_pairs_incl", index=False)
    pairs_pre_split_excl.to_excel(writer, sheet_name="PRE_split_pairs_excl", index=False)
    pairs_pre_uni_incl.to_excel(writer, sheet_name="PRE_unified_pairs_incl", index=False)
    pairs_pre_uni_excl.to_excel(writer, sheet_name="PRE_unified_pairs_excl", index=False)

    # Confusion matrices for NMI
    cm_nmi_split_incl.to_excel(writer, sheet_name="CM_NMI_split_incl")
    cm_nmi_uni_incl.to_excel(writer, sheet_name="CM_NMI_unified_incl")
    cm_nmi_split_excl.to_excel(writer, sheet_name="CM_NMI_split_excl")
    cm_nmi_uni_excl.to_excel(writer, sheet_name="CM_NMI_unified_excl")

    # Confusion matrices for Coherence
    cm_coh_split_incl.to_excel(writer, sheet_name="CM_COH_split_incl")
    cm_coh_uni_incl.to_excel(writer, sheet_name="CM_COH_unified_incl")
    cm_coh_split_excl.to_excel(writer, sheet_name="CM_COH_split_excl")
    cm_coh_uni_excl.to_excel(writer, sheet_name="CM_COH_unified_excl")

print(f"\nSaved results to: {OUT_XLSX}")
print(f"Folder: {OUT_DIR}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import os

# Configurazione estetica
plt.style.use('seaborn-v0_8-muted')
fig, axs = plt.subplots(2, 2, figsize=(14, 12))
((ax1, ax2), (ax3, ax4)) = axs

labels = ['Split Prompts', 'Unified Prompt']
x = np.arange(len(labels))
width = 0.35

# --- DATA PREPARATION ---
nmi_incl, nmi_excl = [0.364, 0.483], [0.591, 0.853]
coh_incl, coh_excl = [0.647, 0.660], [0.876, 0.960]
cov_incl, cov_excl = [0.816, 0.613], [0.546, 0.307]
vio_incl, vio_excl = [47, 34], [11, 2]

# --- 1. NMI (Top Left) ---
ax1.bar(x - width/2, nmi_incl, width, label='Incl. Hybrid', color='#aec7e8')
ax1.bar(x + width/2, nmi_excl, width, label='Excl. Hybrid', color='#1f77b4')
ax1.set_title('A. Statistical Coupling (NMI)', fontsize=13, fontweight='bold')
ax1.set_xticks(x); ax1.set_xticklabels(labels); ax1.set_ylim(0, 1.0)
ax1.legend()

# --- 2. COHERENCE (Top Right) ---
ax2.bar(x - width/2, coh_incl, width, label='Incl. Hybrid', color='#ffbb78')
ax2.bar(x + width/2, coh_excl, width, label='Excl. Hybrid', color='#ff7f0e')
ax2.set_title('B. Logical Coherence (Family Match)', fontsize=13, fontweight='bold')
ax2.set_xticks(x); ax2.set_xticklabels(labels); ax2.set_ylim(0, 1.1)
ax2.legend()

# --- 3. COVERAGE (Bottom Left) ---
ax3.bar(x - width/2, cov_incl, width, label='Incl. Hybrid', color='#98df8a')
ax3.bar(x + width/2, cov_excl, width, label='Excl. Hybrid', color='#2ca02c')
ax3.set_title('C. Data Coverage (Evaluable Rows)', fontsize=13, fontweight='bold')
ax3.set_xticks(x); ax3.set_xticklabels(labels); ax3.set_ylim(0, 1.0)
ax3.legend()

# --- 4. VIOLATIONS (Bottom Right) ---
ax4.bar(x - width/2, vio_incl, width, label='Incl. Hybrid', color='#ff9999')
ax4.bar(x + width/2, vio_excl, width, label='Excl. Hybrid', color='#d62728')
ax4.set_title('D. Categorization Violations (Count)', fontsize=13, fontweight='bold')
ax4.set_xticks(x); ax4.set_xticklabels(labels); ax4.set_ylim(0, 55)
ax4.legend()

# Aggiunta etichette testuali per i dati core (Excl. Hybrid)
for i, v in enumerate(nmi_excl): ax1.text(i + width/2, v + 0.02, f'{v:.2f}', ha='center', fontweight='bold')
for i, v in enumerate(coh_excl): ax2.text(i + width/2, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')
for i, v in enumerate(cov_excl): ax3.text(i + width/2, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')
for i, v in enumerate(vio_excl): ax4.text(i + width/2, v + 1, str(v), ha='center', fontweight='bold')

plt.suptitle('Evolution of Prompting Strategies: Performance and Logical Reliability', fontsize=18, y=1.02)
plt.tight_layout()

# Salvataggio
plt.savefig(os.path.join(OUT_DIR, "full_metrics_grid_2x2.png"), dpi=300, bbox_inches='tight')
plt.show()

"""# Discussion of Results: Prompt Evolution Performance

The analysis of the **163 common events** in the GDELT dataset reveals a clear trend: moving from separate classification tasks (Split) to a single integrated context (Unified) dramatically improves the AI's internal logic.

## 1. Statistical Strength (NMI Analysis)
The **Normalized Mutual Information (NMI)** shows a substantial increase when switching to the Unified prompt:
* **Excluding Hybrid cases**, NMI jumps from **0.59** to **0.85** ($\Delta = +0.26$).
* This indicates that the Unified prompt creates a much stronger statistical "lock" between the macro-category and the specific threat sub-type. The model is far less likely to assign a sub-type that contradicts its primary category.

## 2. Logical Reliability (Coherence & Violations)
The **Coherence Score** (the percentage of cases matching our expert keyword families) confirms this improvement:
* **Accuracy Peak**: When excluding complex hybrid cases, the Unified prompt achieves **96% coherence**, compared to 87.6% for the Split approach.
* **Error Reduction**: The absolute number of categorical violations (e.g., calling a "phishing" event "Kinetic") drops significantly in the Unified mode, reaching as few as **2 violations** in the non-hybrid subset.

## 3. The "Hybrid" Trade-off and Coverage
While the Unified prompt is more accurate, we observe a lower **Data Coverage** (Evaluable Rows):
* **Split Coverage**: ~54.6%
* **Unified Coverage**: ~30.7%
* **Interpretation**: The Unified prompt is more conservative; it tends to produce "cleaner" and more certain classifications, whereas the Split prompts force labels more often but with higher noise and lower logical consistency.

## Conclusion
The **Unified Prompt strategy** is the superior evolution for this pipeline. By providing the model with the full taxonomy in a single context window, we successfully reduced categorical hallucinations and achieved a high-reliability dataset ($NMI \approx 0.85$) suitable for further downstream analysis. Hybridity is too general and is here confused as "combination/mix of threats", so in the next step we applied a clear distinction between technical and non-technical threats. Furthermore, the drop in coverage suggested:

1. That a more comprehensive taxonomy would be necessary. This led us to use ENISA taxonomy for technical threats and a preliminary mapping of non-technical threats - excluding definitions as they are sensitive and still to be implemented.
2. Not all events might contain enough information for the model to perform classification, so we employed an abstention strategy. This will be closely aligned to the preliminary linguistic analysis which was also perfromed in relation to claim presence in the texts - as writing illogic texts can also be a disinformation strategy
"""



"""# Evaluation of Prompting Strategies: ENISA-Aligned Metrics

The previosu pre-ENISA tests led to considering using an official sector-specific taxonomy to improve terminology and logical coherence. For this reason we used the ENISA SPace Threat taxonomy for technical threats, while we also injected a list of non-technical threats now referred as Hybrid Threats (as they are the ones from the CORE-Model). Also, we implemented an abstention strategy to avoid overlabelling when the news text does not contain enough claims. In fact, future tests will also show how claim absence can be a dinformation strategy itself.

Here we evaluate the performance of different prompt strategies against the ENISA Space Threat Taxonomy. The goal is to quantify the alignment between AI-generated classifications and official technical standards, without validating truthfulness of predictions, as a golden standard would be needed.

We deliberately do not report NMI between threat categories and classes, as the ENISA taxonomy defines a one-to-many semantic relationship between categories and classes. In this setting, NMI becomes sensitive to label cardinality rather than to logical correctness. Instead, we rely on explicit taxonomy-coherence measures, which directly assess whether category–class assignments respect the official ENISA mapping.

# 1. Terminological Harmonization

We measure how consistently the model uses terms defined in the official taxonomies.

ENISA Harmonization: Percentage of rows where the threat_category belongs to the official ENISA set.

Hybrid Vocab Harmonization: Percentage of rows where the HT (Hybrid Threat) field matches the predefined hybrid threat vocabulary.

# 2. Taxonomy Coherence ($C$)

This metric evaluates if the predicted Category ($Cat$) and Class ($Cl$) pair is logically valid according to the ENISA framework.$$C_{pair} = \frac{1}{N_{p}} \sum_{i=1}^{N_{p}} \mathbb{I} \left( (Cat_i, Cl_i) \in \mathcal{T}_{allowed} \right)$$Where $\mathcal{T}_{allowed}$ is the set of official taxonomy pairs, $N_p$ is the total number of identified pairs, and $\mathbb{I}$ is the indicator function.

# 3. Segment Violation Rate ($V_{seg}$)

It assesses the physical/logical feasibility of the threat. If a specific threat class is technically restricted to the Ground Segment but is flagged on the Space Segment, it is counted as a violation.$$V_{seg} = \frac{\sum \text{events where } (S_{active} \cap S_{allowed} = \emptyset)}{\text{Total Mappable Events}}$$

# 4. Abstention & Constraint Violation (CVR)

We evaluate the model's ability to "abstain" when information is insufficient.Skip Agreement: $P(\text{skip}=1 \mid \text{NotEnough})$. Probability that the model activates the skip flag when information is detected as insufficient.CVR (Violation | Not Enough): Measures if the model incorrectly provides technical classifications despite stating there is not enough information.

# Legend of Symbols and Variables

$$
\begin{array}{|l|l|}
\hline
\mathbf{Symbol} & \mathbf{Description} \\ \hline
\mathcal{T}_{allowed} & \text{The reference set of valid (Category, Class) mappings from ENISA.} \\ \hline
N_p & \text{Total number of Category-Class pairs detected across all rows.} \\ \hline
Cat & \text{The high-level Threat Category (e.g., Physical Attack).} \\ \hline
Cl & \text{The specific Threat Class (e.g., Jamming, Malware).} \\ \hline
V_{seg} & \text{Violation rate indicating technical misalignment with space segments.} \\ \hline
S_{active} & \text{The segments flagged by the model (Space, Ground, User, Link).} \\ \hline
S_{allowed} & \text{The segments permitted by the taxonomy for a specific Class.} \\ \hline
\emptyset & \text{Empty set (indicating no overlap between predicted and allowed segments).} \\ \hline
\end{array}
$$
"""

!pip -q install pandas openpyxl numpy

import os
import re
import pandas as pd
import numpy as np
from datetime import datetime

# =========================
# OUTPUT FOLDER (Colab)
# =========================
OUT_DIR = "/content/prompt strategies evolution evaluation tables"
os.makedirs(OUT_DIR, exist_ok=True)

# =========================
# PATHS
# =========================
PBI_XLSX    = "/content/files_for_PBI.xlsx"
TAXO_CSV    = "/content/Space_Threat_Taxonomy.csv"
HYBRID_XLSX = "/content/Hybrid Threats.xlsx"

# =========================
# BASIC HELPERS
# =========================
def norm_key(x):
    if pd.isna(x): return ""
    return re.sub(r"\s+", " ", str(x).strip().lower()).strip()

# robust: remove punctuation to make column/class matching resilient
def norm_label(x):
    if pd.isna(x): return ""
    s = str(x).strip().lower()
    s = re.sub(r"[^a-z0-9]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def is_empty_like(x):
    s = "" if pd.isna(x) else str(x).strip().lower()
    return s == "" or s in {"none","n/a","na","null","nan","-"} or "not applicable" in s

def is_not_enough(x):
    s = "" if pd.isna(x) else str(x).strip().lower()
    return "not enough" in s

def is_none_attack(x):
    s = "" if pd.isna(x) else str(x).strip().lower()
    return s == "" or s == "none"

# =========================
# LOAD TAXONOMY (ENISA technical)
# =========================
tax = pd.read_csv(TAXO_CSV)

TAXO_CAT_COL   = "threat_category"
TAXO_CLASS_COL = "threat_class"
TAXO_ASSET_COL = "affected_asset"

for c in [TAXO_CAT_COL, TAXO_CLASS_COL, TAXO_ASSET_COL]:
    if c not in tax.columns:
        raise ValueError(f"Missing column '{c}' in taxonomy CSV. Found: {list(tax.columns)}")

tax["cat_key"]   = tax[TAXO_CAT_COL].apply(norm_key)
tax["class_key"] = tax[TAXO_CLASS_COL].apply(norm_key)

valid_categories = set(tax["cat_key"].dropna().unique())
valid_classes    = set(tax["class_key"].dropna().unique())
allowed_pairs    = set(zip(tax["cat_key"], tax["class_key"]))  # official (category,class) pairs

# (category,class)->allowed segments
SEG_COLS = ["Space Segment","Ground Segment","HR_segment","User Segment"]

def assets_to_segset(s):
    s = "" if pd.isna(s) else str(s).lower()
    out = set()
    if "space" in s: out.add("Space Segment")
    if "ground" in s: out.add("Ground Segment")
    if "uplink" in s or "downlink" in s or "link" in s or "communication" in s:
        out.add("HR_segment")
    if "user" in s: out.add("User Segment")
    return out

tax["allowed_seg_set"] = tax[TAXO_ASSET_COL].apply(assets_to_segset)
tax_map = (
    tax.groupby(["cat_key","class_key"])["allowed_seg_set"]
       .apply(lambda sets: set().union(*sets))
       .to_dict()
)

print("Parsed taxonomy pairs (category,class):", len(allowed_pairs))
print("Valid categories:", len(valid_categories), "| Valid classes:", len(valid_classes))

# Map taxonomy "display class name" -> class_key via robust norm_label
# This lets us match dataframe columns (which are class names) even if punctuation/case differs.
tax_class_by_normlabel = {}
for disp in tax[TAXO_CLASS_COL].dropna().astype(str).unique():
    tax_class_by_normlabel[norm_label(disp)] = norm_key(disp)

# =========================
# LOAD HYBRID VOCAB
# =========================
hy = pd.read_excel(HYBRID_XLSX)
hy_col = "Hybrid Threats"
if hy_col not in hy.columns:
    raise ValueError(f"Missing column '{hy_col}' in Hybrid Threats file. Found: {list(hy.columns)}")

hybrid_vocab = set(
    hy[hy_col].dropna().astype(str).str.strip().str.lower().tolist()
)
print("Hybrid vocab size:", len(hybrid_vocab))

def ht_vocab_any_ok(x):
    # "any-term in vocab" harmonization
    if is_empty_like(x):
        return True
    parts = [p.strip() for p in str(x).lower().split(",") if p.strip()]
    if not parts:
        return True
    return any(p in hybrid_vocab for p in parts)

# =========================
# LOAD DATA (FULL SHEETS)
# =========================
gd = pd.read_excel(PBI_XLSX, sheet_name="GDELT")
em = pd.read_excel(PBI_XLSX, sheet_name="EMM")

def ensure_cols(d, dname, cols):
    miss = [c for c in cols if c not in d.columns]
    if miss:
        raise ValueError(f"[{dname}] Missing columns: {miss}")

# sanity: segments exist
ensure_cols(gd, "GDELT", SEG_COLS)
ensure_cols(em, "EMM", SEG_COLS)

# =========================
# DETECT CLASS BINARY COLS (ENISA-aligned)
# =========================
META_COLS = set([
    "ID","globaleventid","guid","event_date","title","description","raw_resp","Detected_langugae",
    "threat_issue","threat_attack","technicality","HT","threat_category","threat_class",
    "Confidence","retrieval_term","enough_info","targeted_segment","vulnerability","capability",
    "Lifecycle","CIA_affected","tone","Polarized","Fictional","Claim","reasoning_category","reasoning_class"
] + SEG_COLS)

def build_class_col_mapping(df):
    """
    Return:
      - class_cols: dataframe columns that are ENISA class binaries (0/1) after robust matching
      - col_to_class_key: dict col_name -> ENISA class_key
    """
    col_to_class_key = {}
    for c in df.columns:
        if c in META_COLS:
            continue
        nl = norm_label(c)
        if nl in tax_class_by_normlabel:
            col_to_class_key[c] = tax_class_by_normlabel[nl]
    class_cols = list(col_to_class_key.keys())
    return class_cols, col_to_class_key

gd_class_cols, gd_col2key = build_class_col_mapping(gd)
em_class_cols, em_col2key = build_class_col_mapping(em)

print("Detected ENISA class binary cols: GDELT_PBI =", len(gd_class_cols))
print("Detected ENISA class binary cols: EMM_PBI   =", len(em_class_cols))

def active_class_keys(row, class_cols, col2key):
    out = []
    for c in class_cols:
        try:
            if int(row[c]) == 1:
                out.append(col2key[c])
        except:
            pass
    return out

def row_segset(row):
    out = set()
    for c in SEG_COLS:
        try:
            if int(row[c]) == 1:
                out.add(c)
        except:
            pass
    return out

# =========================
# METRICS
# =========================
def compute_metrics(df, name, class_cols, col2key):
    d = df.copy()

    # ----- Terminological harmonization
    cat_key = d["threat_category"].apply(norm_key) if "threat_category" in d.columns else ""
    enisa_vocab_ok = float(np.mean([is_empty_like(x) or (norm_key(x) in valid_categories) for x in d.get("threat_category", pd.Series([""]*len(d)))]))

    ht_vocab_ok = float(np.mean([ht_vocab_any_ok(x) for x in d.get("HT", pd.Series([""]*len(d)))]))

    # ----- Coherence (category<->class) using official taxonomy pairs
    event_eval = 0
    event_ok   = 0
    pair_eval  = 0
    pair_ok    = 0

    for _, r in d.iterrows():
        cat = r.get("threat_category","")
        catk = norm_key(cat)
        if is_empty_like(cat) or catk not in valid_categories:
            continue

        act = active_class_keys(r, class_cols, col2key)
        if not act:
            continue

        event_eval += 1
        all_ok = True
        for ck in act:
            pair_eval += 1
            if (catk, ck) in allowed_pairs:
                pair_ok += 1
            else:
                all_ok = False
        if all_ok:
            event_ok += 1

    coh_event = (event_ok / event_eval) if event_eval else np.nan
    coh_pair  = (pair_ok  / pair_eval)  if pair_eval  else np.nan

    # ----- Segment violation (taxonomy-driven)
    seg_eval = 0
    seg_viol = 0
    for _, r in d.iterrows():
        cat = r.get("threat_category","")
        catk = norm_key(cat)
        if is_empty_like(cat) or catk not in valid_categories:
            continue

        act = active_class_keys(r, class_cols, col2key)
        if not act:
            continue

        segs = row_segset(r)

        for ck in act:
            allowed_segs = tax_map.get((catk, ck), set())
            if not allowed_segs:
                continue
            seg_eval += 1
            bad = (len(segs) == 0) or (len(segs & allowed_segs) == 0)
            if bad:
                seg_viol += 1

    seg_violation_rate = (seg_viol / seg_eval) if seg_eval else np.nan

    # ----- Abstention rules
    # Not enough => must be skip==1 and no ENISA class active
    ne_mask = d["enough_info"].apply(is_not_enough) if "enough_info" in d.columns else pd.Series([False]*len(d))
    ne_N = int(ne_mask.sum())

    if "skip" in d.columns and ne_N > 0:
        skip_agree = float(np.mean(d.loc[ne_mask, "skip"].fillna(0).astype(int) == 1))
        ne_viol = 0
        for _, r in d.loc[ne_mask].iterrows():
            act = active_class_keys(r, class_cols, col2key)
            sk = 0
            try:
                sk = int(r.get("skip", 0))
            except:
                sk = 0
            if (sk != 1) or (len(act) > 0):
                ne_viol += 1
        cvr_cond  = ne_viol / ne_N
        cvr_joint = ne_viol / len(d)
    else:
        skip_agree = np.nan
        cvr_cond   = np.nan
        cvr_joint  = np.nan

    # None => no technical outputs (here: no category AND no ENISA classes)
    if "threat_attack" in d.columns:
        none_mask = d["threat_attack"].apply(is_none_attack)
    else:
        none_mask = pd.Series([False]*len(d))

    none_N = int(none_mask.sum())
    none_viol = 0
    for _, r in d.loc[none_mask].iterrows():
        cat_present = not is_empty_like(r.get("threat_category",""))
        act_present = len(active_class_keys(r, class_cols, col2key)) > 0
        if cat_present or act_present:
            none_viol += 1
    cvr_none = (none_viol / none_N) if none_N else np.nan

    return {
        "dataset": name,
        "N_rows": int(len(d)),
        "ENISA_vocab_harmonization": round(enisa_vocab_ok, 3),
        "HT_vocab_harmonization": round(ht_vocab_ok, 3),
        "Taxonomy_coherence_event_level(cat↔class)": round(coh_event, 3) if not np.isnan(coh_event) else np.nan,
        "Taxonomy_coherence_pair_level(cat↔class)": round(coh_pair, 3) if not np.isnan(coh_pair) else np.nan,
        "N_mappable_pairs(cat,class)": int(pair_eval),
        "Segment_violation_rate": round(seg_violation_rate, 3) if not np.isnan(seg_violation_rate) else np.nan,
        "N_mappable_seg_pairs": int(seg_eval),
        "SkipAgreement_P(skip|NotEnough)": round(skip_agree, 3) if not np.isnan(skip_agree) else np.nan,
        "CVR_cond_P(Violation|NotEnough)": round(cvr_cond, 3) if not np.isnan(cvr_cond) else np.nan,
        "CVR_joint_P(NotEnough&Violation)": round(cvr_joint, 3) if not np.isnan(cvr_joint) else np.nan,
        "N_not_enough": int(ne_N),
        "CVR_none⇒no_technical_outputs": round(cvr_none, 3) if not np.isnan(cvr_none) else np.nan,
        "N_none": int(none_N),
        # debug (useful if you ever see "0 subset")
        "_debug_event_eval_rows": int(event_eval),
    }

# =========================
# RUN
# =========================
rows = [
    compute_metrics(gd, "GDELT_PBI", gd_class_cols, gd_col2key),
    compute_metrics(em, "EMM_PBI", em_class_cols, em_col2key),
]
summary = pd.DataFrame(rows)

print("\n=== Summary (robust label matching) ===")
display(summary)

# =========================
# SAVE OUTPUTS
# =========================
ts = datetime.now().strftime("%Y%m%d_%H%M%S")
xlsx_out = os.path.join(OUT_DIR, f"post_enisa_core_metrics_{ts}.xlsx")
csv_out  = os.path.join(OUT_DIR, f"post_enisa_core_metrics_{ts}.csv")

summary.to_excel(xlsx_out, index=False)
summary.to_csv(csv_out, index=False)

print("\nSaved files:")
print("-", xlsx_out)
print("-", csv_out)
print("Folder:", OUT_DIR)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os

# Configurazione cartella di output
OUT_DIR = "/content/prompt strategies evolution evaluation tables"
os.makedirs(OUT_DIR, exist_ok=True)

# Dati aggiornati (rimossa Taxo Coherence Pair)
data = {
    'Metric': ['ENISA Vocab', 'HT Vocab', 'Taxo Coherence', 'Segment Reliability', 'Skip Agreement'],
    'GDELT_PBI': [0.887, 0.988, 0.965, 1-0.046, 1.0],
    'EMM_PBI': [0.972, 0.995, 0.992, 1-0.155, 1.0]
}

df_plot = pd.DataFrame(data)

# Setup stile
plt.style.use('seaborn-v0_8-whitegrid')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))

width = 0.35

# --- GRAFICO 1: Alignment & Coherence ---
x1 = np.arange(3) # ENISA Vocab, HT Vocab, Taxo Coherence
ax1.bar(x1 - width/2, df_plot.iloc[:3]['GDELT_PBI'], width, label='GDELT (N=6k)', color='#3498db', alpha=0.8)
ax1.bar(x1 + width/2, df_plot.iloc[:3]['EMM_PBI'], width, label='EMM (N=7k)', color='#e74c3c', alpha=0.8)

ax1.set_title('Terminological & Structural Alignment', fontsize=14, fontweight='bold', pad=30)
ax1.set_xticks(x1)
ax1.set_xticklabels(df_plot.iloc[:3]['Metric'], fontsize=11)
ax1.set_ylim(0.8, 1.05)
# Legenda in alto
ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=2, frameon=True)

# --- GRAFICO 2: Governance & Reliability ---
x2 = np.arange(2) # Segment Reliability, Skip Agreement
ax2.bar(x2 - width/2, df_plot.iloc[3:]['GDELT_PBI'], width, color='#3498db', alpha=0.8)
ax2.bar(x2 + width/2, df_plot.iloc[3:]['EMM_PBI'], width, color='#e74c3c', alpha=0.8)

ax2.set_title('Governance & Operational Reliability', fontsize=14, fontweight='bold', pad=30)
ax2.set_xticks(x2)
ax2.set_xticklabels(df_plot.iloc[3:]['Metric'], fontsize=11)
ax2.set_ylim(0.8, 1.05)

# Annotazione Perfect Abstention
ax2.annotate('Perfect Abstention (1.0)', xy=(1, 1.0), xytext=(0.6, 1.03),
             arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=5),
             fontsize=10, fontweight='bold')

plt.tight_layout()

# Salvataggio
save_path = os.path.join(OUT_DIR, "scaling_final_evaluation.png")
plt.savefig(save_path, dpi=300, bbox_inches='tight')
plt.show()

print(f"Grafico salvato in: {save_path}")

"""# Discussion of Results: Evaluation Metrics Analysis

Following the execution of the validation framework, we present the performance comparison between the GDELT_PBI ($N \approx 6,700$) and EMM_PBI ($N \approx 7,400$) datasets.1. Terminological & Structural AlignmentThe alignment with official vocabularies is consistently high across both datasets, ensuring data interoperability:ENISA Harmonization: EMM demonstrates superior alignment ($97.2\%$) compared to GDELT ($88.7\%$). This suggests that the EMM source data adheres more strictly to ENISA category naming conventions.Hybrid Threat (HT) Vocab: Both datasets achieve near-perfect scores ($>98.8\%$), indicating that the terminology for hybrid threats is well-integrated.Taxonomy Coherence: Both datasets show excellent internal logic. At the pair level ($Cat \leftrightarrow Cl$), EMM reaches $99.3\%$ coherence, while GDELT maintains a strong $97.0\%$.2. Segment Reliability and ViolationsThis metric acts as a technical "sanity check" for the physical or logical location of the threat:GDELT Reliability: Shows a very low violation rate ($4.6\%$), meaning identified threats are almost always assigned to technically compatible segments (Space, Ground, User, or Link).EMM Reliability: Exhibits a higher violation rate ($15.5\%$). This suggests EMM reports may contain more ambiguous technical descriptions that challenge the model’s mapping to the ENISA segment taxonomy.3. Governance and Operational ReliabilityThe model's ability to abstain from providing false information is measured via the Constraint Violation Rate (CVR) and Skip Agreement.$$\begin{array}{|l|c|c|}
\hline
\mathbf{Metric} & \mathbf{GDELT\_PBI} & \mathbf{EMM\_PBI} \\ \hline
\text{Skip Agreement } P(skip|NotEnough) & 100\% & 100\% \\ \hline
\text{CVR (Violation | Not Enough)} & 0.1\% & 0.0\% \\ \hline
\text{CVR (No Tech Output | None Attack)} & 1.4\% & 0.1\% \\ \hline
\end{array}$$Perfect Abstention: Both datasets achieve a Skip Agreement of 1.0 (100%). When the model detects "Not Enough Info," it correctly triggers the skip flag.Low Hallucination Risk: The extremely low CVR scores indicate high model discipline; it does not "hallucinate" technical classes when information is declared insufficient.Legend of Symbols for Results$$\begin{array}{|l|l|}
\hline
\mathbf{Symbol} & \mathbf{Description} \\ \hline
N & \text{Total number of rows/events analyzed in the dataset.} \\ \hline
P(skip \mid NE) & \text{Probability of skipping an event when info is insufficient.} \\ \hline
CVR_{cond} & \text{Violation Rate: providing tech data despite a "Not Enough Info" label.} \\ \hline
Cat \leftrightarrow Cl & \text{Logical consistency between a Threat Category and its specific Class.} \\ \hline
\end{array}$$
"""

import os
from google.colab import files

# 1. Definisci il percorso della cartella
folder_path = '/content/prompt strategies evolution evaluation tables'

# 2. Definisci il nome del file zip in uscita
zip_filename = 'evaluation_tables_analysis.zip'

# 3. Controlla se la cartella esiste prima di procedere
if os.path.exists(folder_path):
    # Comprime la cartella usando il comando shell 'zip'
    # -r serve per includere ricorsivamente le sottocartelle
    # -q serve per non intasare l'output del log (quiet)
    !zip -r -q "{zip_filename}" "{folder_path}"

    print(f"✅ Cartella compressa con successo in: {zip_filename}")

    # 4. Avvia il download nel browser
    files.download(zip_filename)
else:
    print(f"❌ Errore: La cartella {folder_path} non è stata trovata.")









