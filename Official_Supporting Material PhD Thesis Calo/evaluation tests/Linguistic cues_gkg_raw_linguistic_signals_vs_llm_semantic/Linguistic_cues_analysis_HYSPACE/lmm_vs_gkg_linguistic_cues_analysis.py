# -*- coding: utf-8 -*-
"""Lmm_vs_GKG_Linguistic_cues_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SLpY29by2d8JwYoDfsj93380PWO6l89q

# Official Convergence Analysis: GKG vs. LLM (Tests 1–4)

### **Objective**
This analysis evaluates the degree of alignment between **GDELT Global Knowledge Graph (GKG)** lexical baselines and **Large Language Model (LLM)** semantic interpretations. We focus on two primary linguistic dimensions: **Tone** and **Polarization**. The goal is to determine if LLM "judgments" can reliably approximate the signal detected by traditional lexical density tools when specific governance layers are applied.

### **Metrics & Selection Rationale**
To quantify convergence, we utilize a multi-layered statistical approach:

* **Polarization Index ($PolIdx$):** A composite baseline metric constructed from GKG data. It integrates **Polarity**, **Self/Group Reference Density**, and **Activity Density**. We selected this composite because polarization is rarely a single-variable signal; it requires a balance of emotional intensity (polarity) and group-based mobilization (activity/identity).
* **ROC-AUC (Area Under the Curve):** Used for polarization convergence. This measures the LLM's ability to distinguish between high and low polarization events as defined by the GKG baseline. An $AUC > 0.80$ indicates strong alignment.
* **Spearman Rank Correlation ($\rho$):** Applied to Tone analysis to assess the monotonic relationship between GKG scores and LLM-derived numeric weights.
* **Mean Absolute Error ($MAE$):** Quantifies the average magnitude of "drift" between the LLM’s tone and the GKG baseline on a normalized $0–1$ scale.
* **Quantile Tests (Mann-Whitney U):** Used specifically for high-variability tests (3 & 4) to determine if LLM tone labels are statistically enriched within the upper or lower extremes of the GKG distribution.

### **Workflow**
The following code standardizes Event IDs across disparate sources, normalizes GKG lexical densities to a robust $0–1$ scale, and executes the comparative loop across four distinct test environments, ranging from highly restricted prompts to interpretative taxonomies.
Three xlsx files to map tone keywords (from LLMs) to tone gkg normalized scores are used for the comparison
"""

# ============================================================
# OFFICIAL CONVERGENCE ANALYSIS — GKG vs LLM (Tests 1–4)
# Focus: Tone & Polarization convergence / divergence
# Outputs saved in linguistic_cues_analysis_HYSPACE
# ============================================================

import os, re
import numpy as np
import pandas as pd
from scipy.stats import spearmanr, mannwhitneyu
from sklearn.metrics import roc_auc_score

# =========================
# 0) OUTPUT FOLDER
# =========================
OUT_DIR = "/content/linguistic_cues_analysis_HYSPACE"
os.makedirs(OUT_DIR, exist_ok=True)

# =========================
# 1) PATHS
# =========================
BASELINE_PATH = "/content/gkg_linguistic_baseline_events (1).csv"

TEST1_2_XLSX  = "/content/2_vs_3_gdelt_emm_complete_space.xlsx"
TEST3_XLSX    = "/content/ENISA_taxonomy_gdelt_emm_official_deepseek.xlsx"
TEST4_XLSX    = "/content/2022_gdelt_official.xlsx"

LEX_XLSX = "/content/tone_lexicon.xlsx"
NEU_XLSX = "/content/tone_neutral_terms.xlsx"
MOD_XLSX = "/content/tone_modifiers.xlsx"

# =========================
# 2) GENERIC HELPERS
# =========================
def to_event_id(x):
    if pd.isna(x): return np.nan
    s = str(x).strip()
    return s[:-2] if re.fullmatch(r"\d+\.0", s) else s

def normalize_colname(s):
    return re.sub(r"[\s\-_]+", "", s.lower().strip())

def find_col(df, candidates):
    lookup = {normalize_colname(c): c for c in df.columns}
    for cand in candidates:
        k = normalize_colname(cand)
        if k in lookup:
            return lookup[k]
    return None

def standardize_event_id(df):
    col = find_col(df, ["event_id","globaleventid","globalEventID","guid","gkgrecordid"])
    if col is None:
        raise KeyError("No event ID column found.")
    df = df.copy()
    df["event_id"] = df[col].apply(to_event_id).astype(str)
    return df

def robust_01(x, p_low=5, p_high=95):
    x = pd.to_numeric(x, errors="coerce")
    lo, hi = np.nanpercentile(x, [p_low, p_high])
    if np.isclose(hi, lo) or np.isnan(lo) or np.isnan(hi):
        return (x - x.min()) / (x.max() - x.min() + 1e-9)
    return ((x - lo) / (hi - lo)).clip(0, 1)

def parse_yesno(text):
    if pd.isna(text): return np.nan
    t = str(text).strip().lower()
    if t.startswith("yes"): return 1
    if t.startswith("no"): return 0
    return np.nan

# =========================
# 3) LOAD TONE LEXICONS
# =========================
lex = pd.read_excel(LEX_XLSX)
neu = pd.read_excel(NEU_XLSX)
mod = pd.read_excel(MOD_XLSX)

lex.columns = lex.columns.str.lower()
neu.columns = neu.columns.str.lower()
mod.columns = mod.columns.str.lower()

# force numeric weights
lex_terms = lex.iloc[:,0].astype(str).str.lower().str.strip()
lex_weights = pd.to_numeric(lex.iloc[:,1], errors="coerce")

LEX = dict(zip(lex_terms, lex_weights))
LEX = {k: v for k, v in LEX.items() if pd.notna(v)}

NEU = set(neu.iloc[:,0].astype(str).str.lower().str.strip())

MODIFIERS = list(
    zip(
        mod.iloc[:,0].astype(str).str.lower().str.strip(),
        pd.to_numeric(mod.iloc[:,1], errors="coerce")
    )
)
MODIFIERS = [(m,f) for m,f in MODIFIERS if pd.notna(f)]


# =========================
# 4) LOAD BASELINE (GKG)
# =========================
gkg = standardize_event_id(pd.read_csv(BASELINE_PATH))

col_tone = find_col(gkg, ["mean_tone"])
col_pol  = find_col(gkg, ["mean_polarity"])
col_self = find_col(gkg, ["mean_self_group_density"])
col_act  = find_col(gkg, ["mean_activity_density"])

# =========================
# 5) LOAD TESTS
# =========================
tests = {
    "test1": standardize_event_id(pd.read_excel(TEST1_2_XLSX, sheet_name="GDELT_1_TYPES")),
    "test2": standardize_event_id(pd.read_excel(TEST1_2_XLSX, sheet_name="GDELT_EVERYHTING_SPECIFIC")),
    "test3": standardize_event_id(pd.read_excel(TEST3_XLSX, sheet_name="ENISA_taxonomy_gdelt_emm_offici")),
    "test4": standardize_event_id(pd.read_excel(TEST4_XLSX))
}

TONE_CANDS = ["language_tone","tone"]
POL_CANDS  = ["polarization","polarized"]

# =========================
# 6) MAIN EVALUATION LOOP
# =========================
summary = []
tone_quantiles = []

for name, df in tests.items():

    tone_col = find_col(df, TONE_CANDS)
    pol_col  = find_col(df, POL_CANDS)

    m = gkg.merge(df, on="event_id", how="left")

    m["Tone_GKG_norm"] = m[col_tone] / 100
    m["Polarity_norm"] = robust_01(m[col_pol])
    m["SelfGroup_norm"] = robust_01(m[col_self])
    m["Activity_norm"] = robust_01(m[col_act])

    m["PolIdx"] = (
        0.5*m["Polarity_norm"] +
        0.3*m["SelfGroup_norm"] +
        0.2*m["Activity_norm"]
    )

    if pol_col:
        m["Polarization_LLM_bin"] = m[pol_col].apply(parse_yesno)

    if tone_col:
        m["Tone_lex_norm"] = m[tone_col].apply(tone_lex_from_label)

    # ---- TONE METRICS ----
    tv = m[["Tone_GKG_norm","Tone_lex_norm"]].dropna()
    rho_tone = spearmanr(tv.iloc[:,0], tv.iloc[:,1])[0] if tv.shape[0]>3 else np.nan
    mae = np.mean(np.abs(tv.iloc[:,0]-tv.iloc[:,1])) if tv.shape[0]>0 else np.nan

    # quantiles for test3/4
    if name in ["test3","test4"] and tv.shape[0]>20:
        q20 = tv["Tone_lex_norm"].quantile(0.2)
        q80 = tv["Tone_lex_norm"].quantile(0.8)
        low = tv[tv["Tone_lex_norm"]<=q20]["Tone_GKG_norm"]
        high = tv[tv["Tone_lex_norm"]>=q80]["Tone_GKG_norm"]
        p = mannwhitneyu(low, high).pvalue
        tone_quantiles.append({
            "test": name,
            "delta_means": high.mean()-low.mean(),
            "p_value": p
        })

    # ---- POLARIZATION METRICS ----
    pv = m[["PolIdx","Polarization_LLM_bin"]].dropna()
    auc = roc_auc_score(pv.iloc[:,1], pv.iloc[:,0]) if pv["Polarization_LLM_bin"].nunique()>1 else np.nan

    summary.append({
        "test": name,
        "tone_spearman": rho_tone,
        "tone_mae": mae,
        "pol_auc": auc,
        "pol_yes_rate": pv["Polarization_LLM_bin"].mean() if len(pv) else np.nan
    })

    m.to_csv(f"{OUT_DIR}/merged_{name}.csv", index=False)

# =========================
# 7) SAVE OUTPUTS
# =========================
summary_df = pd.DataFrame(summary)
summary_df.to_excel(f"{OUT_DIR}/summary_convergence.xlsx", index=False)

pd.DataFrame(tone_quantiles).to_excel(
    f"{OUT_DIR}/tone_quantile_tests.xlsx", index=False
)

print("Saved outputs to:", OUT_DIR)
display(summary_df)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os

# 1) Define Output Path
OUT_DIR = "/content/linguistic_cues_analysis_HYSPACE"
if not os.path.exists(OUT_DIR):
    os.makedirs(OUT_DIR)

# 2) Prepare the data
data = {
    "test": ["Test 1", "Test 2", "Test 3", "Test 4"],
    "pol_auc": [0.806081, 0.816176, 0.558359, 0.473775],
    "pol_yes_rate": [0.063291, 0.049689, 0.582822, 0.549020]
}
df = pd.DataFrame(data)

# 3) Plotting configuration
x = np.arange(len(df["test"]))
width = 0.35
fig, ax = plt.subplots(figsize=(12, 7), dpi=120)

# 4) Create the bars
# Blue: Alignment Strength
rects1 = ax.bar(x - width/2, df["pol_auc"], width,
                label='Convergence Reliability (AUC): Higher indicates stronger alignment with GKG',
                color='#2c7bb6', edgecolor='white', linewidth=1)

# Orange: Frequency (The higher this is, the more the model "over-labels")
rects2 = ax.bar(x + width/2, df["pol_yes_rate"], width,
                label='Labeling Saturation (Yes Rate): Higher indicates broader LLM sensitivity/bias',
                color='#fdae61', edgecolor='white', linewidth=1)

# 5) Professional Styling
ax.set_ylabel('Normalized Metric Score', fontsize=12, fontweight='bold')
ax.set_title('Polarization Convergence Analysis: GKG Baseline vs. LLM Tests', fontsize=14, pad=25)
ax.set_xticks(x)
ax.set_xticklabels(df["test"], fontsize=11)
ax.set_ylim(0, 1.1)

# Statistical baseline
ax.axhline(0.5, color='black', linestyle='--', alpha=0.3, label='Random Chance Baseline (0.50 AUC)')

# 6) Value Annotations
def annotate_bars(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 5),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

annotate_bars(rects1)
annotate_bars(rects2)

# 7) Legend and Grid
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False, fontsize=10)
ax.grid(axis='y', linestyle=':', alpha=0.5)
plt.tight_layout()

# 8) Save and Show
save_path = os.path.join(OUT_DIR, "LLM_vs_GKG_polarization_convergence.png")
plt.savefig(save_path, bbox_inches='tight')
print(f"Visualization saved to: {save_path}")
plt.show()

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os

# 1) Define Output Path
OUT_DIR = "/content/linguistic_cues_analysis_HYSPACE"
if not os.path.exists(OUT_DIR):
    os.makedirs(OUT_DIR)

# 2) Data based on your summary results for Test 3 and 4
# We use 0 for Spearman Rho to visually represent NaN (no correlation)
data_tone = {
    "test": ["Test 3", "Test 4"],
    "tone_spearman": [0, 0],  # Represents no correlation (NaN)
    "tone_mae": [0.026435, 0.026435]
}
df_tone = pd.DataFrame(data_tone)

# 3) Plotting configuration
x = np.arange(len(df_tone["test"]))
fig, ax1 = plt.subplots(figsize=(10, 6), dpi=120)

# 4) Bar Chart for Mean Absolute Error (MAE)
color_mae = '#95a5a6' # Professional grey for error
rects1 = ax1.bar(x, df_tone["tone_mae"], width=0.5,
                 label='Tone MAE (Mean Absolute Error): Constant Discrepancy',
                 color=color_mae, alpha=0.6, edgecolor='black')

# 5) Line Chart for Correlation (Spearman Rho)
ax2 = ax1.twinx() # Create a second y-axis
ax2.plot(x, df_tone["tone_spearman"], color='#e74c3c', marker='X', markersize=12,
         linestyle='--', linewidth=2, label='Tone Spearman Rho: No Correlation (Critical Divergence)')

# 6) Professional Styling and Labels (English)
ax1.set_xlabel('Test Phases', fontsize=12)
ax1.set_ylabel('Mean Absolute Error (MAE)', color=color_mae, fontsize=12, fontweight='bold')
ax2.set_ylabel('Spearman Correlation Coefficient', color='#e74c3c', fontsize=12, fontweight='bold')
ax1.set_title('Tone Alignment Stagnation: GKG vs. LLM (Tests 3 & 4)', fontsize=14, pad=25)

ax1.set_xticks(x)
ax1.set_xticklabels(df_tone["test"])
ax1.set_ylim(0, 0.05) # Zoom in to show the flatness of the MAE
ax2.set_ylim(-0.1, 0.1) # Focus on the near-zero correlation

# Add a "Zero Correlation" line
ax2.axhline(0, color='#e74c3c', linestyle='-', alpha=0.3)

# 7) Add annotation for critical divergence
ax1.annotate('Critical Divergence: Zero Correlation',
             xy=(0.5, 0), xytext=(0.5, 0.03),
             arrowprops=dict(facecolor='black', shrink=0.05),
             ha='center', fontsize=10, bbox=dict(boxstyle="round,pad=0.3", fc="w", ec="gray", lw=0.5))

# 8) Unified Legend
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc='upper center', bbox_to_anchor=(0.5, -0.15), frameon=False)

# 9) Save and Show
save_path_tone_divergence = os.path.join(OUT_DIR, "LLM_vs_GKG_tone_divergence_T3_T4.png")
plt.tight_layout()
plt.savefig(save_path_tone_divergence, bbox_inches='tight')
print(f"Tone divergence plot for Tests 3 & 4 saved to: {save_path_tone_divergence}")
plt.show()

"""## **Convergence Discussion: The Polarization Signal vs. Tone Drift**

### **1. Polarization as a Stable Cross-Layer Signal**
The analysis reveals a clear distinction between restricted and interpretative modeling. In **Tests 1 and 2**, which employed restrictive prompts explicitly aligned with GKG-style definitions, we observe **strong convergence** ($AUC \approx 0.81$).

* **Insight:** When LLM outputs are tightly constrained and conceptually mapped to GKG-style constructs, semantic interpretations converge toward stable lexical baselines.
* **Significance:** This alignment suggests **behavioral consistency** across layers. While it does not imply "truth," it demonstrates that polarization can act as a reliable cross-layer signal if governance mechanisms are applied to the prompt layer.

### **2. Interpretative Drift in Tests 3 and 4**
In contrast, **Tests 3 and 4** exhibit a collapse in convergence ($AUC$ dropping toward $0.50$ and significantly higher "Yes" rates).

* **Observation:** The LLM became labelled over 50% of events as polarized. This divergence reinforces a key methodological insight: **prompt constraining** is a necessary condition for comparability. Without explicit mapping rules, the LLM drifts toward interpretative bias, losing alignment with the lexical silver standard.



### **3. Tone: Limited Alignment and Structural Constraints**
Tone-related comparisons do not exhibit the same stability as polarization.

* **Stagnation:** Across all tests, the **Spearman Correlation ($\rho$) remains at zero (NaN)**, and the **MAE is constant ($\approx 0.026$)**.
* **Diagnosis:** In restrictive settings, tone outputs are too neutral for meaningful comparison. In interpretative settings, tone is susceptible to **interpretative drift**—it is a composite, context-sensitive construct that lacks a stable cross-layer mapping rule in this current iteration.

### **Conclusion: The Governance Requirement**
Tone is treated in this analysis as a **secondary cue**, whereas polarization serves as the primary convergence signal. These findings suggest that a **governance-enabling layer** is required not only for terminological coherence but to define the specific mapping rules between context-approached linguistic signals (GKG) and LLM predictions.

**Convergence**, in this framework, serves as a **confidence index** to be assessed by human analysts: high convergence indicates a reliable automated proxy, while low convergence (as seen in Tests 3 & 4) flags a need for human intervention or prompt recalibration.

However, divergence is also a meaningful index.

# Strategic Framing Analysis: Semantic–Lexical Divergence

### **Objective**
Rather than attempting to reconcile semantic and lexical disagreement, this analysis reframes **divergence itself as a useful operational signal for security analysts**. By identifying cases where surface language remains neutral while the underlying content is polarized, we can detect potential **Strategic Framing**.

### **Methodology: The Quadrant Logic**
We jointly consider three primary linguistic dimensions to categorize events into four analytically interpretable quadrants:
1.  **Lexical Neutrality:** Defined as $|Tone_{GKG}| < 0.02$.
2.  **LLM Polarization Detection:** Binary classification (Yes/No) from the model.
3.  **Semantic Polarization Intensity ($PolIdx$):** The weighted baseline metric.

The primary target is the **"Possible Framing"** quadrant, where $Tone \approx 0$ but $Polarization = 1$. This identifies narratives that use "sterile" or factual-sounding language to deliver highly divisive or polarized messaging.

### **Workflow**
The following script processes the merged datasets from Tests 1 and 2, as they rpoved to be the most reliable in relation to polarization convergence with gkg. It calculates the **Framing Score**, and generates a triage summary to demonstrate how these combined cues function as a workload reduction mechanism.
The obteined Framing score is just a preliminary example of how context+content approaches, governed by knowledge approaches and governance, could create an attention index for analysts. Certainly, other indexes could be further created
"""

import pandas as pd
import os

OUT_DIR = "/content/linguistic_cues_analysis_HYSPACE"
os.makedirs(OUT_DIR, exist_ok=True)

# Load already-built framing dataset (tests 1–2)
df = pd.read_csv(f"{OUT_DIR}/framing_test1_test2_full.csv")

# =========================
# 1) Quadrant counts
# =========================
quadrant_counts = (
    df["Quadrant"]
    .value_counts(dropna=False)
    .reset_index()
)
quadrant_counts.columns = ["Quadrant", "Count"]

print("\nQuadrant counts:")
display(quadrant_counts)

# =========================
# 2) Before vs After table
# =========================
before_after = pd.DataFrame([
    {"Stage": "Before (no prioritization)", "Events to review": len(df)},
    {"Stage": "Neutral + Not polarized (safe)",
     "Events to review": int((df["Quadrant"]=="Neutral language + Not polarized (LLM)").sum())},
    {"Stage": "Non-neutral + Not polarized (style/emphasis)",
     "Events to review": int((df["Quadrant"]=="Non-neutral language + Not polarized (LLM)").sum())},
    {"Stage": "Non-neutral + Polarized (explicit)",
     "Events to review": int((df["Quadrant"]=="Non-neutral language + Polarized (LLM)").sum())},
    {"Stage": "Neutral + Polarized (POSSIBLE FRAMING)",
     "Events to review": int((df["Quadrant"].str.contains("POSSIBLE FRAMING")).sum())},
])

print("\nBefore vs After (analyst workload):")
display(before_after)

# =========================
# 3) Top possible framing cases (table only)
# =========================
possible_framing = (
    df[df["Quadrant"].str.contains("POSSIBLE FRAMING")]
    .sort_values("FramingScore_cont", ascending=False)
)

cols = [
    "event_id","test","Tone_GKG_norm","PolIdx","Polarization_LLM_bin",
    "FramingScore_cont","Quadrant","reason","title"
]
cols = [c for c in cols if c in possible_framing.columns]

print("\nPossible framing candidates (to review):")
display(possible_framing[cols])

# =========================
# 4) Save everything to Excel
# =========================
out_xlsx = f"{OUT_DIR}/framing_summary_tables.xlsx"
with pd.ExcelWriter(out_xlsx) as writer:
    quadrant_counts.to_excel(writer, sheet_name="Quadrant_counts", index=False)
    before_after.to_excel(writer, sheet_name="Before_vs_After", index=False)
    possible_framing[cols].to_excel(writer, sheet_name="Possible_framing_cases", index=False)

print("\nSaved tables to:", out_xlsx)

"""## **Analyst-Oriented Results & Triage Logic**

### **1. Quadrant Distribution: Separating Signal from Noise**
As shown in the output, the majority of events do not require immediate attention. The distribution into quadrants serves as a primary filter:

| Quadrant | Count | Analyst Action |
| :--- | :--- | :--- |
| **Non-neutral + Not polarized** | 180 | Contextualized as stylistic emphasis, to review with less urgency. |
| **Neutral + Not polarized** | 123 | Safest category; layers converge. |
| **Non-neutral + Polarized** | 11 | Explicit polarization (low deception). |
| **Neutral + Polarized (POSSIBLE FRAMING)** | **7** | **High Priority: Strategic framing candidate. Human assessment is necessary** |

### **2. Workload Reduction & Efficiency**
From an analyst-centric perspective, this approach yields a substantial reduction in cognitive load:
* **Initial Volume:** 326 events.
* **Triage Result:** Only **7 events** (~2%) emerge as high-priority candidates for framing assessment.
* **Outcome:** 123 events are safely deprioritized, and 180 are contextualized as narrative style rather than strategic manipulation.



### **3. Qualitative Validation: Framing vs. Factual Reporting**
A qualitative inspection of the top-ranked cases (e.g., the *SpaceX Starlink* delivery to Ukraine) highlights the nuance of this system:
* **The Signal:** The system flagged the Starlink case due to high semantic polarization ($PolIdx \approx 0.64$) despite a perfectly neutral lexical tone ($Tone = 0.00$).
* **The Human Verdict:** Manual review reveals factual reporting of geopolitical tension rather than deceptive manipulation.
* **Conclusion:** Semantic–lexical divergence is not a "verdict" of disinformation, but a high-precision **invitation for analyst assessment**. It identifies *where* the tension lies, even when the language used to describe it is intentionally sterilized.

### **Summary of Top Framing Candidates**
The generated table `top_framing_candidates_test1_test2.xlsx` provides the final prioritized list, ranking events by their **Framing Score**, calculated as:
$$FramingScore_{cont} = (1 - |Tone_{GKG}|) \times PolIdx$$
This ensures that the most "neutral-sounding" yet "semantically-heavy" cases are reviewed first.

Again, this is just a preliminary example to be further investigated, together with other indexes, after human evaluation
"""

import shutil
from google.colab import files

# Define the folder to be zipped and the output filename
folder_path = '/content/linguistic_cues_analysis_HYSPACE'
zip_filename = 'linguistic_cues_analysis_HYSPACE.zip'

# 1. Create the zip archive
# This takes the content of folder_path and compresses it into zip_filename
shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', folder_path)

# 2. Trigger the download to your local machine
files.download(zip_filename)

print(f"Archive {zip_filename} created and download triggered.")











